# Phase 4: æ•°æ®åŒæ­¥ç³»ç»Ÿ

**ä¼˜å…ˆçº§**: ğŸŸ¡ ä¸­
**çŠ¶æ€**: â¬œ å¾…å¼€å§‹
**é¢„è®¡å·¥ä½œé‡**: ä¸­ç­‰
**ä¾èµ–**: Phase 1C å®Œæˆ

---

## ä»»åŠ¡æ¸…å•

### â¬œ Task 1: Celery ä»»åŠ¡é˜Ÿåˆ—é…ç½®
**çŠ¶æ€**: å¾…å¼€å§‹
**æ–‡ä»¶**:
- åˆ›å»º: `backend/app/core/celery_app.py`
- ä¿®æ”¹: `backend/requirements.txt`
- ä¿®æ”¹: `docker-compose.yml`

**æ­¥éª¤**:

1. **æ·»åŠ  Celery ä¾èµ–**
   ```python
   # backend/requirements.txt
   celery==5.3.6
   redis==5.0.1
   flower==2.0.1  # Celery ç›‘æ§å·¥å…·
   ```

2. **é…ç½® Celery åº”ç”¨**
   ```python
   # backend/app/core/celery_app.py

   from celery import Celery
   from app.core.config import settings

   celery_app = Celery(
       "stock_ai",
       broker=f"redis://{settings.REDIS_HOST}:{settings.REDIS_PORT}/0",
       backend=f"redis://{settings.REDIS_HOST}:{settings.REDIS_PORT}/0"
   )

   celery_app.conf.update(
       task_serializer='json',
       accept_content=['json'],
       result_serializer='json',
       timezone='Asia/Shanghai',
       enable_utc=True,
       task_track_started=True,
       task_time_limit=300,  # 5 åˆ†é’Ÿè¶…æ—¶
       worker_prefetch_multiplier=1,
       worker_max_tasks_per_child=1000
   )

   # è‡ªåŠ¨å‘ç°ä»»åŠ¡
   celery_app.autodiscover_tasks(['app.tasks'])
   ```

3. **æ·»åŠ  Celery Worker åˆ° Docker Compose**
   ```yaml
   # docker-compose.yml

   celery_worker:
     build: ./backend
     command: celery -A app.core.celery_app worker --loglevel=info
     volumes:
       - ./backend:/app
     depends_on:
       - redis
       - postgres
     environment:
       - REDIS_HOST=redis
       - POSTGRES_HOST=postgres

   celery_beat:
     build: ./backend
     command: celery -A app.core.celery_app beat --loglevel=info
     volumes:
       - ./backend:/app
     depends_on:
       - redis
     environment:
       - REDIS_HOST=redis

   flower:
     build: ./backend
     command: celery -A app.core.celery_app flower --port=5555
     ports:
       - "5555:5555"
     depends_on:
       - redis
       - celery_worker
   ```

4. **å¯åŠ¨ Celery**
   ```bash
   docker-compose up -d celery_worker celery_beat flower
   ```

5. **æäº¤ä»£ç **
   ```bash
   git add backend/app/core/celery_app.py docker-compose.yml
   git commit -m "feat: add Celery task queue configuration"
   ```

---

### â¬œ Task 2: å®æ—¶è¡Œæƒ…åŒæ­¥ä»»åŠ¡
**çŠ¶æ€**: å¾…å¼€å§‹
**æ–‡ä»¶**:
- åˆ›å»º: `backend/app/tasks/data_sync.py`
- åˆ›å»º: `backend/tests/unit/test_data_sync.py`

**æ­¥éª¤**:

1. **å®ç°å®æ—¶è¡Œæƒ…åŒæ­¥ä»»åŠ¡**
   ```python
   # backend/app/tasks/data_sync.py

   from celery import shared_task
   from app.services.data_service import DataService
   from app.core.cache import get_redis
   from app.core.database import get_influxdb
   import asyncio

   @shared_task(name="sync_realtime_quotes")
   def sync_realtime_quotes():
       """åŒæ­¥å®æ—¶è¡Œæƒ…ï¼ˆäº¤æ˜“æ—¶é—´æ¯ 3 ç§’æ‰§è¡Œï¼‰"""
       asyncio.run(_sync_realtime_quotes())

   async def _sync_realtime_quotes():
       data_service = DataService()
       redis = get_redis()
       influxdb = get_influxdb()

       # 1. è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
       stock_codes = await data_service.get_all_stock_codes()

       # 2. æ‰¹é‡è·å–å®æ—¶è¡Œæƒ…
       quotes = await data_service.fetch_realtime_quotes_batch(stock_codes)

       # 3. å†™å…¥ Redisï¼ˆTTL: 5ç§’ï¼‰
       for quote in quotes:
           cache_key = f"stock:realtime:{quote['stock_code']}"
           await redis.setex(cache_key, 5, json.dumps(quote))

       # 4. å†™å…¥ InfluxDB
       await influxdb.write_realtime_quotes(quotes)

       return len(quotes)
   ```

2. **é…ç½®å®šæ—¶ä»»åŠ¡**
   ```python
   # backend/app/core/celery_app.py

   from celery.schedules import crontab

   celery_app.conf.beat_schedule = {
       'sync-realtime-quotes': {
           'task': 'sync_realtime_quotes',
           'schedule': 3.0,  # æ¯ 3 ç§’
           'options': {
               'expires': 2.0  # 2 ç§’åè¿‡æœŸ
           }
       }
   }
   ```

3. **æ·»åŠ äº¤æ˜“æ—¶é—´åˆ¤æ–­**
   ```python
   def is_trading_time() -> bool:
       """åˆ¤æ–­æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´"""
       now = datetime.now()
       weekday = now.weekday()

       # å‘¨æœ«ä¸äº¤æ˜“
       if weekday >= 5:
           return False

       # äº¤æ˜“æ—¶é—´: 9:30-11:30, 13:00-15:00
       time_now = now.time()
       morning_start = time(9, 30)
       morning_end = time(11, 30)
       afternoon_start = time(13, 0)
       afternoon_end = time(15, 0)

       return (morning_start <= time_now <= morning_end) or \
              (afternoon_start <= time_now <= afternoon_end)

   @shared_task(name="sync_realtime_quotes")
   def sync_realtime_quotes():
       if not is_trading_time():
           return "Not trading time"

       asyncio.run(_sync_realtime_quotes())
   ```

4. **æäº¤ä»£ç **
   ```bash
   git add backend/app/tasks/data_sync.py
   git commit -m "feat: add realtime quotes sync task"
   ```

---

### â¬œ Task 3: å†å²æ•°æ®åŒæ­¥ä»»åŠ¡
**çŠ¶æ€**: å¾…å¼€å§‹
**æ–‡ä»¶**:
- ä¿®æ”¹: `backend/app/tasks/data_sync.py`

**æ­¥éª¤**:

1. **å®ç° K çº¿æ•°æ®åŒæ­¥**
   ```python
   @shared_task(name="sync_kline_data")
   def sync_kline_data(stock_code: str, period: str = '1d'):
       """åŒæ­¥ K çº¿æ•°æ®"""
       asyncio.run(_sync_kline_data(stock_code, period))

   async def _sync_kline_data(stock_code: str, period: str):
       data_service = DataService()
       influxdb = get_influxdb()

       # è·å– K çº¿æ•°æ®ï¼ˆæœ€è¿‘ 500 å¤©ï¼‰
       kline_data = await data_service.fetch_kline_data(
           stock_code, period=period, days=500
       )

       # å†™å…¥ InfluxDB
       await influxdb.write_kline_data(stock_code, period, kline_data)

       return len(kline_data)
   ```

2. **å®ç°è´¢åŠ¡æ•°æ®åŒæ­¥**
   ```python
   @shared_task(name="sync_financial_data")
   def sync_financial_data(stock_code: str):
       """åŒæ­¥è´¢åŠ¡æ•°æ®"""
       asyncio.run(_sync_financial_data(stock_code))

   async def _sync_financial_data(stock_code: str):
       data_service = DataService()
       db = get_db()

       # è·å–è´¢åŠ¡æ•°æ®ï¼ˆæœ€è¿‘ 5 å¹´ï¼‰
       financials = await data_service.fetch_financial_data(stock_code, years=5)

       # å†™å…¥ PostgreSQL
       for financial in financials:
           db.merge(StockFinancial(**financial))
       db.commit()

       return len(financials)
   ```

3. **å®ç°æ‰¹é‡åŒæ­¥ä»»åŠ¡**
   ```python
   @shared_task(name="sync_all_stocks_data")
   def sync_all_stocks_data():
       """æ‰¹é‡åŒæ­¥æ‰€æœ‰è‚¡ç¥¨æ•°æ®"""
       asyncio.run(_sync_all_stocks_data())

   async def _sync_all_stocks_data():
       data_service = DataService()
       stock_codes = await data_service.get_all_stock_codes()

       # ä½¿ç”¨ Celery group å¹¶è¡Œæ‰§è¡Œ
       from celery import group
       job = group(
           sync_kline_data.s(code) for code in stock_codes
       )
       result = job.apply_async()

       return f"Syncing {len(stock_codes)} stocks"
   ```

4. **é…ç½®æ¯æ—¥åŒæ­¥ä»»åŠ¡**
   ```python
   # backend/app/core/celery_app.py

   celery_app.conf.beat_schedule.update({
       'sync-all-stocks-daily': {
           'task': 'sync_all_stocks_data',
           'schedule': crontab(hour=16, minute=0),  # æ¯å¤© 16:00
       }
   })
   ```

5. **æäº¤ä»£ç **
   ```bash
   git add backend/app/tasks/data_sync.py backend/app/core/celery_app.py
   git commit -m "feat: add historical data sync tasks"
   ```

---

### â¬œ Task 4: æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ä»»åŠ¡
**çŠ¶æ€**: å¾…å¼€å§‹
**æ–‡ä»¶**:
- åˆ›å»º: `backend/app/tasks/indicator_calc.py`

**æ­¥éª¤**:

1. **å®ç°æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ä»»åŠ¡**
   ```python
   # backend/app/tasks/indicator_calc.py

   @shared_task(name="calculate_indicators")
   def calculate_indicators(stock_code: str):
       """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡"""
       asyncio.run(_calculate_indicators(stock_code))

   async def _calculate_indicators(stock_code: str):
       from app.utils.indicators import (
           calculate_ma, calculate_macd, calculate_rsi, calculate_kdj
       )

       influxdb = get_influxdb()

       # 1. è·å– K çº¿æ•°æ®
       kline_data = await influxdb.read_kline_data(stock_code, period='1d', days=120)

       if len(kline_data) < 60:
           return "Insufficient data"

       df = pd.DataFrame(kline_data)

       # 2. è®¡ç®—å„ç±»æŒ‡æ ‡
       ma_data = calculate_ma(df['close'], [5, 10, 20, 60])
       macd_data = calculate_macd(df['close'])
       rsi_data = calculate_rsi(df['close'], [6, 12, 24])
       kdj_data = calculate_kdj(df['high'], df['low'], df['close'])

       # 3. å†™å…¥ InfluxDB
       indicators = {
           **ma_data,
           **macd_data,
           **rsi_data,
           **kdj_data
       }

       await influxdb.write_indicators(stock_code, indicators)

       return "Indicators calculated"
   ```

2. **é…ç½®æŒ‡æ ‡è®¡ç®—ä»»åŠ¡**
   ```python
   # backend/app/core/celery_app.py

   celery_app.conf.beat_schedule.update({
       'calculate-indicators-daily': {
           'task': 'calculate_all_indicators',
           'schedule': crontab(hour=16, minute=30),  # æ¯å¤© 16:30
       }
   })
   ```

3. **æäº¤ä»£ç **
   ```bash
   git add backend/app/tasks/indicator_calc.py
   git commit -m "feat: add technical indicator calculation tasks"
   ```

---

### â¬œ Task 5: å®¹é”™å’Œé‡è¯•æœºåˆ¶
**çŠ¶æ€**: å¾…å¼€å§‹
**æ–‡ä»¶**:
- ä¿®æ”¹: `backend/app/tasks/data_sync.py`
- åˆ›å»º: `backend/app/utils/retry.py`

**æ­¥éª¤**:

1. **å®ç°é‡è¯•è£…é¥°å™¨**
   ```python
   # backend/app/utils/retry.py

   from functools import wraps
   import time

   def retry_on_failure(max_retries=3, delay=1, backoff=2):
       """é‡è¯•è£…é¥°å™¨"""
       def decorator(func):
           @wraps(func)
           async def wrapper(*args, **kwargs):
               retries = 0
               current_delay = delay

               while retries < max_retries:
                   try:
                       return await func(*args, **kwargs)
                   except Exception as e:
                       retries += 1
                       if retries >= max_retries:
                           raise

                       print(f"Retry {retries}/{max_retries} after {current_delay}s: {e}")
                       time.sleep(current_delay)
                       current_delay *= backoff

               return None
           return wrapper
       return decorator
   ```

2. **æ·»åŠ ä»»åŠ¡é‡è¯•é…ç½®**
   ```python
   # backend/app/tasks/data_sync.py

   @shared_task(
       name="sync_realtime_quotes",
       bind=True,
       max_retries=3,
       default_retry_delay=60
   )
   def sync_realtime_quotes(self):
       try:
           if not is_trading_time():
               return "Not trading time"

           asyncio.run(_sync_realtime_quotes())
       except Exception as exc:
           # é‡è¯•ä»»åŠ¡
           raise self.retry(exc=exc, countdown=60)
   ```

3. **æ·»åŠ é”™è¯¯æ—¥å¿—**
   ```python
   import logging

   logger = logging.getLogger(__name__)

   @shared_task(name="sync_realtime_quotes")
   def sync_realtime_quotes():
       try:
           asyncio.run(_sync_realtime_quotes())
       except Exception as e:
           logger.error(f"Failed to sync realtime quotes: {e}", exc_info=True)
           raise
   ```

4. **æäº¤ä»£ç **
   ```bash
   git add backend/app/tasks/ backend/app/utils/retry.py
   git commit -m "feat: add retry mechanism and error handling for tasks"
   ```

---

## å®Œæˆæ ‡å‡†

Phase 4 å®Œæˆåï¼Œæ•°æ®åŒæ­¥ç³»ç»Ÿåº”å…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š

### åŠŸèƒ½å®Œæ•´æ€§
- âœ… Celery ä»»åŠ¡é˜Ÿåˆ—é…ç½®
- âœ… å®æ—¶è¡Œæƒ…åŒæ­¥ï¼ˆäº¤æ˜“æ—¶é—´æ¯ 3 ç§’ï¼‰
- âœ… å†å² K çº¿æ•°æ®åŒæ­¥
- âœ… è´¢åŠ¡æ•°æ®åŒæ­¥
- âœ… æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
- âœ… å®šæ—¶ä»»åŠ¡è°ƒåº¦

### è´¨é‡æ ‡å‡†
- âœ… ä»»åŠ¡æ‰§è¡ŒæˆåŠŸç‡ > 95%
- âœ… é”™è¯¯é‡è¯•æœºåˆ¶å®Œå–„
- âœ… æ—¥å¿—è®°å½•å®Œæ•´

### æ€§èƒ½æ ‡å‡†
- âœ… å®æ—¶è¡Œæƒ…åŒæ­¥å»¶è¿Ÿ < 5s
- âœ… æ‰¹é‡åŒæ­¥æ”¯æŒå¹¶å‘
- âœ… ä»»åŠ¡é˜Ÿåˆ—ä¸ç§¯å‹

---

## ä¸‹ä¸€æ­¥

å®Œæˆ Phase 4 åï¼Œè¿›å…¥ **Phase 5: å‰ç«¯åº”ç”¨å¼€å‘**

å‚è€ƒæ–‡æ¡£: `docs/tasks/phase-5-frontend.md`
