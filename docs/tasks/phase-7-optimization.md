# Phase 7: ç¼“å­˜ä¼˜åŒ–ä¸æ€§èƒ½è°ƒä¼˜

**ä¼˜å…ˆçº§**: ğŸŸ¢ ä½
**çŠ¶æ€**: â¬œ å¾…å¼€å§‹
**é¢„è®¡å·¥ä½œé‡**: ä¸­ç­‰
**ä¾èµ–**: Phase 1C, Phase 2 å®Œæˆ

---

## ä»»åŠ¡æ¸…å•

### â¬œ Task 1: å¤šçº§ç¼“å­˜æ¶æ„å®ç°
**çŠ¶æ€**: å¾…å¼€å§‹
**æ–‡ä»¶**:
- åˆ›å»º: `backend/app/core/cache_manager.py`

**æ­¥éª¤**:

1. **å®ç°å¤šçº§ç¼“å­˜ç®¡ç†å™¨**
   ```python
   # backend/app/core/cache_manager.py

   from typing import Optional, Any
   import json

   class CacheManager:
       """å¤šçº§ç¼“å­˜ç®¡ç†å™¨"""

       def __init__(self, redis_client, local_cache_size: int = 1000):
           self.redis = redis_client
           self.local_cache = {}  # æœ¬åœ°å†…å­˜ç¼“å­˜
           self.local_cache_size = local_cache_size

       async def get(self, key: str) -> Optional[Any]:
           """è·å–ç¼“å­˜ï¼ˆå…ˆæœ¬åœ°ï¼Œå Redisï¼‰"""
           # L1: æœ¬åœ°å†…å­˜ç¼“å­˜
           if key in self.local_cache:
               return self.local_cache[key]

           # L2: Redis ç¼“å­˜
           value = await self.redis.get(key)
           if value:
               data = json.loads(value)
               # å›å¡«æœ¬åœ°ç¼“å­˜
               self._set_local(key, data)
               return data

           return None

       async def set(
           self,
           key: str,
           value: Any,
           ttl: int = 3600,
           local_ttl: int = 60
       ):
           """è®¾ç½®ç¼“å­˜ï¼ˆåŒæ—¶å†™å…¥æœ¬åœ°å’Œ Redisï¼‰"""
           # å†™å…¥ Redis
           await self.redis.setex(key, ttl, json.dumps(value))

           # å†™å…¥æœ¬åœ°ç¼“å­˜
           self._set_local(key, value)

       def _set_local(self, key: str, value: Any):
           """è®¾ç½®æœ¬åœ°ç¼“å­˜ï¼ˆLRU æ·˜æ±°ï¼‰"""
           if len(self.local_cache) >= self.local_cache_size:
               # åˆ é™¤æœ€æ—§çš„é¡¹
               oldest_key = next(iter(self.local_cache))
               del self.local_cache[oldest_key]

           self.local_cache[key] = value

       async def delete(self, key: str):
           """åˆ é™¤ç¼“å­˜"""
           # åˆ é™¤æœ¬åœ°ç¼“å­˜
           if key in self.local_cache:
               del self.local_cache[key]

           # åˆ é™¤ Redis ç¼“å­˜
           await self.redis.delete(key)

       async def clear_local(self):
           """æ¸…ç©ºæœ¬åœ°ç¼“å­˜"""
           self.local_cache.clear()
   ```

2. **åº”ç”¨åˆ°æœåŠ¡å±‚**
   ```python
   # backend/app/services/stock_service.py

   from app.core.cache_manager import CacheManager

   class StockService:
       def __init__(self):
           self.cache = CacheManager(redis_client)

       async def get_stock_info(self, stock_code: str):
           cache_key = f"stock:info:{stock_code}"

           # å°è¯•ä»ç¼“å­˜è·å–
           cached = await self.cache.get(cache_key)
           if cached:
               return cached

           # ä»æ•°æ®åº“æŸ¥è¯¢
           stock = await self._fetch_from_db(stock_code)

           # å†™å…¥ç¼“å­˜
           await self.cache.set(cache_key, stock, ttl=3600)

           return stock
   ```

3. **æäº¤ä»£ç **
   ```bash
   git add backend/app/core/cache_manager.py
   git commit -m "feat: implement multi-level cache architecture"
   ```

---

### â¬œ Task 2: ç¼“å­˜é¢„çƒ­ç­–ç•¥
**çŠ¶æ€**: å¾…å¼€å§‹
**æ–‡ä»¶**:
- åˆ›å»º: `backend/app/tasks/cache_warmup.py`

**æ­¥éª¤**:

1. **å®ç°ç¼“å­˜é¢„çƒ­ä»»åŠ¡**
   ```python
   # backend/app/tasks/cache_warmup.py

   from celery import shared_task
   from app.services.data_service import DataService
   from app.core.cache_manager import CacheManager

   @shared_task(name="warmup_hot_stocks")
   def warmup_hot_stocks():
       """é¢„çƒ­çƒ­é—¨è‚¡ç¥¨æ•°æ®"""
       asyncio.run(_warmup_hot_stocks())

   async def _warmup_hot_stocks():
       data_service = DataService()
       cache = CacheManager(redis_client)

       # è·å–çƒ­é—¨è‚¡ç¥¨åˆ—è¡¨ï¼ˆæˆäº¤é‡å‰ 100ï¼‰
       hot_stocks = await data_service.get_hot_stocks(limit=100)

       # æ‰¹é‡é¢„çƒ­
       for stock in hot_stocks:
           # é¢„çƒ­å®æ—¶è¡Œæƒ…
           realtime = await data_service.fetch_realtime_quote(stock['code'])
           await cache.set(
               f"stock:realtime:{stock['code']}",
               realtime,
               ttl=5
           )

           # é¢„çƒ­åŸºç¡€ä¿¡æ¯
           info = await data_service.fetch_stock_info(stock['code'])
           await cache.set(
               f"stock:info:{stock['code']}",
               info,
               ttl=3600
           )

       return len(hot_stocks)
   ```

2. **é…ç½®å®šæ—¶é¢„çƒ­**
   ```python
   # backend/app/core/celery_app.py

   celery_app.conf.beat_schedule.update({
       'warmup-hot-stocks': {
           'task': 'warmup_hot_stocks',
           'schedule': crontab(minute='*/30'),  # æ¯ 30 åˆ†é’Ÿ
       }
   })
   ```

3. **æäº¤ä»£ç **
   ```bash
   git add backend/app/tasks/cache_warmup.py
   git commit -m "feat: add cache warmup strategy"
   ```

---

### â¬œ Task 3: æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–
**çŠ¶æ€**: å¾…å¼€å§‹
**æ–‡ä»¶**:
- ä¿®æ”¹: `backend/app/models/*.py`
- åˆ›å»º: `backend/alembic/versions/xxx_add_indexes.py`

**æ­¥éª¤**:

1. **æ·»åŠ æ•°æ®åº“ç´¢å¼•**
   ```python
   # backend/alembic/versions/xxx_add_indexes.py

   def upgrade():
       # è‚¡ç¥¨è¡¨ç´¢å¼•
       op.create_index('idx_stocks_industry', 'stocks', ['industry'])
       op.create_index('idx_stocks_market_cap', 'stocks', ['market_cap'])

       # è´¢åŠ¡æ•°æ®è¡¨ç´¢å¼•
       op.create_index('idx_financials_roe', 'stock_financials', ['roe'])
       op.create_index('idx_financials_pe', 'stock_financials', ['pe_ttm'])

       # ç­–ç•¥æ‰§è¡Œå†å²ç´¢å¼•
       op.create_index(
           'idx_executions_user_time',
           'strategy_executions',
           ['user_id', 'executed_at']
       )

   def downgrade():
       op.drop_index('idx_stocks_industry')
       op.drop_index('idx_stocks_market_cap')
       op.drop_index('idx_financials_roe')
       op.drop_index('idx_financials_pe')
       op.drop_index('idx_executions_user_time')
   ```

2. **ä¼˜åŒ–æŸ¥è¯¢è¯­å¥**
   ```python
   # backend/app/services/stock_service.py

   # ä¼˜åŒ–å‰
   stocks = db.query(Stock).filter(Stock.pe < 15).all()

   # ä¼˜åŒ–åï¼ˆä½¿ç”¨ç´¢å¼• + åˆ†é¡µï¼‰
   stocks = db.query(Stock)\
       .filter(Stock.pe < 15)\
       .order_by(Stock.market_cap.desc())\
       .limit(100)\
       .all()
   ```

3. **ä½¿ç”¨æŸ¥è¯¢ç¼“å­˜**
   ```python
   from sqlalchemy.orm import lazyload

   # é¿å… N+1 æŸ¥è¯¢
   stocks = db.query(Stock)\
       .options(lazyload(Stock.financials))\
       .filter(Stock.pe < 15)\
       .all()
   ```

4. **æäº¤ä»£ç **
   ```bash
   alembic revision --autogenerate -m "add database indexes"
   alembic upgrade head
   git add backend/alembic/
   git commit -m "perf: add database indexes for query optimization"
   ```

---

### â¬œ Task 4: API å“åº”æ—¶é—´ä¼˜åŒ–
**çŠ¶æ€**: å¾…å¼€å§‹
**æ–‡ä»¶**:
- ä¿®æ”¹: `backend/app/api/v1/*.py`

**æ­¥éª¤**:

1. **æ·»åŠ å“åº”æ—¶é—´ä¸­é—´ä»¶**
   ```python
   # backend/main.py

   import time
   from fastapi import Request

   @app.middleware("http")
   async def add_process_time_header(request: Request, call_next):
       start_time = time.time()
       response = await call_next(request)
       process_time = time.time() - start_time
       response.headers["X-Process-Time"] = str(process_time)
       return response
   ```

2. **å®ç°å¹¶å‘æ•°æ®è·å–**
   ```python
   # backend/app/engines/analyzer.py

   import asyncio

   async def analyze(self, stock_code: str):
       # å¹¶å‘è·å–æ•°æ®
       realtime_task = self._get_realtime_data(stock_code)
       kline_task = self._get_kline_data(stock_code)
       financial_task = self._get_financial_data(stock_code)

       realtime, kline, financial = await asyncio.gather(
           realtime_task,
           kline_task,
           financial_task
       )

       # å¹¶å‘æ‰§è¡Œåˆ†æ
       fundamental_task = self._analyze_fundamental(financial)
       technical_task = self._analyze_technical(kline)

       fundamental, technical = await asyncio.gather(
           fundamental_task,
           technical_task
       )

       return self._generate_report(fundamental, technical)
   ```

3. **ä½¿ç”¨è¿æ¥æ± **
   ```python
   # backend/app/core/database.py

   from sqlalchemy import create_engine
   from sqlalchemy.pool import QueuePool

   engine = create_engine(
       DATABASE_URL,
       poolclass=QueuePool,
       pool_size=20,  # è¿æ¥æ± å¤§å°
       max_overflow=10,  # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
       pool_pre_ping=True,  # è¿æ¥å¥åº·æ£€æŸ¥
       pool_recycle=3600  # è¿æ¥å›æ”¶æ—¶é—´
   )
   ```

4. **æäº¤ä»£ç **
   ```bash
   git add backend/
   git commit -m "perf: optimize API response time"
   ```

---

### â¬œ Task 5: æ€§èƒ½ç›‘æ§å’Œåˆ†æ
**çŠ¶æ€**: å¾…å¼€å§‹
**æ–‡ä»¶**:
- åˆ›å»º: `backend/app/core/profiler.py`

**æ­¥éª¤**:

1. **å®ç°æ€§èƒ½åˆ†æè£…é¥°å™¨**
   ```python
   # backend/app/core/profiler.py

   import time
   import logging
   from functools import wraps

   logger = logging.getLogger(__name__)

   def profile(func):
       """æ€§èƒ½åˆ†æè£…é¥°å™¨"""
       @wraps(func)
       async def wrapper(*args, **kwargs):
           start_time = time.time()
           result = await func(*args, **kwargs)
           elapsed = time.time() - start_time

           if elapsed > 1.0:  # è¶…è¿‡ 1 ç§’è®°å½•è­¦å‘Š
               logger.warning(
                   f"Slow function: {func.__name__} took {elapsed:.2f}s"
               )

           return result
       return wrapper
   ```

2. **åº”ç”¨åˆ°å…³é”®å‡½æ•°**
   ```python
   # backend/app/engines/analyzer.py

   from app.core.profiler import profile

   class StockAnalyzer:
       @profile
       async def analyze(self, stock_code: str):
           # åˆ†æé€»è¾‘
           pass
   ```

3. **æ·»åŠ æ€§èƒ½æŒ‡æ ‡æ”¶é›†**
   ```python
   # backend/app/core/metrics.py

   from prometheus_client import Counter, Histogram

   # è¯·æ±‚è®¡æ•°å™¨
   request_count = Counter(
       'api_requests_total',
       'Total API requests',
       ['method', 'endpoint', 'status']
   )

   # å“åº”æ—¶é—´ç›´æ–¹å›¾
   request_duration = Histogram(
       'api_request_duration_seconds',
       'API request duration',
       ['method', 'endpoint']
   )
   ```

4. **æäº¤ä»£ç **
   ```bash
   git add backend/app/core/profiler.py backend/app/core/metrics.py
   git commit -m "feat: add performance monitoring and profiling"
   ```

---

## å®Œæˆæ ‡å‡†

Phase 7 å®Œæˆåï¼Œç³»ç»Ÿæ€§èƒ½åº”è¾¾åˆ°ä»¥ä¸‹æ ‡å‡†ï¼š

### æ€§èƒ½æŒ‡æ ‡
- âœ… API å¹³å‡å“åº”æ—¶é—´ < 500ms
- âœ… ç¼“å­˜å‘½ä¸­ç‡ > 80%
- âœ… æ•°æ®åº“æŸ¥è¯¢æ—¶é—´ < 100ms
- âœ… å¹¶å‘æ”¯æŒ 100+ è¯·æ±‚/ç§’

### ä¼˜åŒ–å®Œæˆåº¦
- âœ… å¤šçº§ç¼“å­˜æ¶æ„å®ç°
- âœ… ç¼“å­˜é¢„çƒ­ç­–ç•¥
- âœ… æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–
- âœ… æŸ¥è¯¢è¯­å¥ä¼˜åŒ–
- âœ… å¹¶å‘å¤„ç†ä¼˜åŒ–
- âœ… æ€§èƒ½ç›‘æ§å®Œå–„

---

## ä¸‹ä¸€æ­¥

å®Œæˆ Phase 7 åï¼Œè¿›å…¥ **Phase 8: ç›‘æ§ã€æ—¥å¿—ä¸éƒ¨ç½²**

å‚è€ƒæ–‡æ¡£: `docs/tasks/phase-8-devops.md`
