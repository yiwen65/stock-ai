# Phase 8: ÁõëÊéß„ÄÅÊó•Âøó‰∏éÈÉ®ÁΩ≤

**‰ºòÂÖàÁ∫ß**: üü¢ ‰Ωé
**Áä∂ÊÄÅ**: ‚¨ú ÂæÖÂºÄÂßã
**È¢ÑËÆ°Â∑•‰ΩúÈáè**: ‰∏≠Á≠â
**‰æùËµñ**: ÊâÄÊúâÊ†∏ÂøÉÂäüËÉΩÂÆåÊàê

---

## ‰ªªÂä°Ê∏ÖÂçï

### ‚¨ú Task 1: ÁªìÊûÑÂåñÊó•ÂøóÁ≥ªÁªü
**Áä∂ÊÄÅ**: ÂæÖÂºÄÂßã
**Êñá‰ª∂**:
- ÂàõÂª∫: `backend/app/core/logging.py`
- ‰øÆÊîπ: `backend/main.py`

**Ê≠•È™§**:

1. **ÈÖçÁΩÆÁªìÊûÑÂåñÊó•Âøó**
   ```python
   # backend/app/core/logging.py

   import logging
   import json
   from datetime import datetime

   class JSONFormatter(logging.Formatter):
       """JSON Ê†ºÂºèÊó•Âøó"""

       def format(self, record):
           log_data = {
               'timestamp': datetime.utcnow().isoformat(),
               'level': record.levelname,
               'logger': record.name,
               'message': record.getMessage(),
               'module': record.module,
               'function': record.funcName,
               'line': record.lineno
           }

           if record.exc_info:
               log_data['exception'] = self.formatException(record.exc_info)

           if hasattr(record, 'user_id'):
               log_data['user_id'] = record.user_id

           if hasattr(record, 'request_id'):
               log_data['request_id'] = record.request_id

           return json.dumps(log_data)

   def setup_logging():
       """ÈÖçÁΩÆÊó•ÂøóÁ≥ªÁªü"""
       # ÂàõÂª∫Êó•ÂøóÂ§ÑÁêÜÂô®
       handler = logging.StreamHandler()
       handler.setFormatter(JSONFormatter())

       # ÈÖçÁΩÆÊ†πÊó•ÂøóÂô®
       root_logger = logging.getLogger()
       root_logger.setLevel(logging.INFO)
       root_logger.addHandler(handler)

       # ÈÖçÁΩÆÂ∫îÁî®Êó•ÂøóÂô®
       app_logger = logging.getLogger('app')
       app_logger.setLevel(logging.DEBUG)

       return app_logger
   ```

2. **Ê∑ªÂä†ËØ∑Ê±ÇÊó•Âøó‰∏≠Èó¥‰ª∂**
   ```python
   # backend/main.py

   import uuid
   from app.core.logging import setup_logging

   logger = setup_logging()

   @app.middleware("http")
   async def log_requests(request: Request, call_next):
       request_id = str(uuid.uuid4())
       request.state.request_id = request_id

       logger.info(
           "Request started",
           extra={
               'request_id': request_id,
               'method': request.method,
               'path': request.url.path,
               'client_ip': request.client.host
           }
       )

       start_time = time.time()
       response = await call_next(request)
       process_time = time.time() - start_time

       logger.info(
           "Request completed",
           extra={
               'request_id': request_id,
               'status_code': response.status_code,
               'process_time': process_time
           }
       )

       return response
   ```

3. **Êèê‰∫§‰ª£Á†Å**
   ```bash
   git add backend/app/core/logging.py backend/main.py
   git commit -m "feat: add structured logging system"
   ```

---

### ‚¨ú Task 2: Prometheus ÁõëÊéßÊåáÊ†á
**Áä∂ÊÄÅ**: ÂæÖÂºÄÂßã
**Êñá‰ª∂**:
- ÂàõÂª∫: `backend/app/core/metrics.py`
- ‰øÆÊîπ: `backend/requirements.txt`
- ‰øÆÊîπ: `backend/main.py`

**Ê≠•È™§**:

1. **Ê∑ªÂä†‰æùËµñ**
   ```python
   # backend/requirements.txt
   prometheus-client==0.19.0
   ```

2. **ÂÆö‰πâÁõëÊéßÊåáÊ†á**
   ```python
   # backend/app/core/metrics.py

   from prometheus_client import Counter, Histogram, Gauge, generate_latest

   # ËØ∑Ê±ÇËÆ°Êï∞Âô®
   http_requests_total = Counter(
       'http_requests_total',
       'Total HTTP requests',
       ['method', 'endpoint', 'status']
   )

   # ËØ∑Ê±ÇÂª∂ËøüÁõ¥ÊñπÂõæ
   http_request_duration_seconds = Histogram(
       'http_request_duration_seconds',
       'HTTP request duration',
       ['method', 'endpoint']
   )

   # Ê¥ªË∑ÉËøûÊé•Êï∞
   active_connections = Gauge(
       'active_connections',
       'Number of active connections'
   )

   # ÁºìÂ≠òÂëΩ‰∏≠Áéá
   cache_hits_total = Counter(
       'cache_hits_total',
       'Total cache hits',
       ['cache_type']
   )

   cache_misses_total = Counter(
       'cache_misses_total',
       'Total cache misses',
       ['cache_type']
   )

   # Êï∞ÊçÆÂ∫ìÊü•ËØ¢
   db_query_duration_seconds = Histogram(
       'db_query_duration_seconds',
       'Database query duration',
       ['query_type']
   )

   # Celery ‰ªªÂä°
   celery_task_duration_seconds = Histogram(
       'celery_task_duration_seconds',
       'Celery task duration',
       ['task_name']
   )

   celery_task_total = Counter(
       'celery_task_total',
       'Total Celery tasks',
       ['task_name', 'status']
   )
   ```

3. **Ê∑ªÂä†ÊåáÊ†áÊî∂ÈõÜ‰∏≠Èó¥‰ª∂**
   ```python
   # backend/main.py

   from app.core.metrics import (
       http_requests_total,
       http_request_duration_seconds,
       active_connections
   )

   @app.middleware("http")
   async def metrics_middleware(request: Request, call_next):
       active_connections.inc()

       start_time = time.time()
       response = await call_next(request)
       duration = time.time() - start_time

       # ËÆ∞ÂΩïÊåáÊ†á
       http_requests_total.labels(
           method=request.method,
           endpoint=request.url.path,
           status=response.status_code
       ).inc()

       http_request_duration_seconds.labels(
           method=request.method,
           endpoint=request.url.path
       ).observe(duration)

       active_connections.dec()

       return response
   ```

4. **Êö¥Èú≤ÊåáÊ†áÁ´ØÁÇπ**
   ```python
   # backend/main.py

   from prometheus_client import generate_latest, CONTENT_TYPE_LATEST

   @app.get("/metrics")
   async def metrics():
       """Prometheus ÊåáÊ†áÁ´ØÁÇπ"""
       return Response(
           content=generate_latest(),
           media_type=CONTENT_TYPE_LATEST
       )
   ```

5. **Êèê‰∫§‰ª£Á†Å**
   ```bash
   git add backend/app/core/metrics.py backend/main.py
   git commit -m "feat: add Prometheus monitoring metrics"
   ```

---

### ‚¨ú Task 3: ÂëäË≠¶Êú∫Âà∂
**Áä∂ÊÄÅ**: ÂæÖÂºÄÂßã
**Êñá‰ª∂**:
- ÂàõÂª∫: `backend/app/core/alerting.py`

**Ê≠•È™§**:

1. **ÂÆûÁé∞ÂëäË≠¶Á≥ªÁªü**
   ```python
   # backend/app/core/alerting.py

   import logging
   from typing import Dict, Any
   from enum import Enum

   logger = logging.getLogger(__name__)

   class AlertLevel(str, Enum):
       INFO = "info"
       WARNING = "warning"
       ERROR = "error"
       CRITICAL = "critical"

   class AlertManager:
       """ÂëäË≠¶ÁÆ°ÁêÜÂô®"""

       def __init__(self):
           self.alert_handlers = []

       def add_handler(self, handler):
           """Ê∑ªÂä†ÂëäË≠¶Â§ÑÁêÜÂô®"""
           self.alert_handlers.append(handler)

       async def send_alert(
           self,
           level: AlertLevel,
           title: str,
           message: str,
           metadata: Dict[str, Any] = None
       ):
           """ÂèëÈÄÅÂëäË≠¶"""
           alert = {
               'level': level,
               'title': title,
               'message': message,
               'metadata': metadata or {},
               'timestamp': datetime.utcnow().isoformat()
           }

           logger.warning(f"Alert: {title} - {message}")

           # Ë∞ÉÁî®ÊâÄÊúâÂ§ÑÁêÜÂô®
           for handler in self.alert_handlers:
               try:
                   await handler.handle(alert)
               except Exception as e:
                   logger.error(f"Alert handler failed: {e}")

   class LogAlertHandler:
       """Êó•ÂøóÂëäË≠¶Â§ÑÁêÜÂô®"""

       async def handle(self, alert: Dict):
           logger.log(
               self._get_log_level(alert['level']),
               f"ALERT: {alert['title']} - {alert['message']}",
               extra=alert['metadata']
           )

       def _get_log_level(self, alert_level: AlertLevel):
           mapping = {
               AlertLevel.INFO: logging.INFO,
               AlertLevel.WARNING: logging.WARNING,
               AlertLevel.ERROR: logging.ERROR,
               AlertLevel.CRITICAL: logging.CRITICAL
           }
           return mapping.get(alert_level, logging.WARNING)

   # ÂÖ®Â±ÄÂëäË≠¶ÁÆ°ÁêÜÂô®
   alert_manager = AlertManager()
   alert_manager.add_handler(LogAlertHandler())
   ```

2. **Â∫îÁî®ÂëäË≠¶**
   ```python
   # backend/app/tasks/data_sync.py

   from app.core.alerting import alert_manager, AlertLevel

   @shared_task(name="sync_realtime_quotes")
   def sync_realtime_quotes():
       try:
           result = asyncio.run(_sync_realtime_quotes())
           return result
       except Exception as e:
           # ÂèëÈÄÅÂëäË≠¶
           asyncio.run(alert_manager.send_alert(
               level=AlertLevel.ERROR,
               title="Data Sync Failed",
               message=f"Failed to sync realtime quotes: {str(e)}",
               metadata={'task': 'sync_realtime_quotes'}
           ))
           raise
   ```

3. **Êèê‰∫§‰ª£Á†Å**
   ```bash
   git add backend/app/core/alerting.py
   git commit -m "feat: add alerting mechanism"
   ```

---

### ‚¨ú Task 4: Áîü‰∫ßÁéØÂ¢É Docker ÈÖçÁΩÆ
**Áä∂ÊÄÅ**: ÂæÖÂºÄÂßã
**Êñá‰ª∂**:
- ÂàõÂª∫: `docker-compose.prod.yml`
- ÂàõÂª∫: `backend/Dockerfile.prod`
- ÂàõÂª∫: `frontend/Dockerfile.prod`
- ÂàõÂª∫: `nginx/nginx.conf`

**Ê≠•È™§**:

1. **ÂàõÂª∫Áîü‰∫ßÁéØÂ¢É Dockerfile**
   ```dockerfile
   # backend/Dockerfile.prod
   FROM python:3.11-slim

   WORKDIR /app

   # ÂÆâË£Ö‰æùËµñ
   COPY requirements.txt .
   RUN pip install --no-cache-dir -r requirements.txt

   # Â§çÂà∂‰ª£Á†Å
   COPY . .

   # ÂàõÂª∫Èùû root Áî®Êà∑
   RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
   USER appuser

   # ÂêØÂä®Â∫îÁî®
   CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
   ```

   ```dockerfile
   # frontend/Dockerfile.prod
   FROM node:18-alpine AS builder

   WORKDIR /app
   COPY package*.json ./
   RUN npm ci
   COPY . .
   RUN npm run build

   FROM nginx:alpine
   COPY --from=builder /app/dist /usr/share/nginx/html
   COPY nginx.conf /etc/nginx/conf.d/default.conf
   EXPOSE 80
   CMD ["nginx", "-g", "daemon off;"]
   ```

2. **ÂàõÂª∫ Nginx ÈÖçÁΩÆ**
   ```nginx
   # nginx/nginx.conf
   upstream backend {
       server backend:8000;
   }

   server {
       listen 80;
       server_name _;

       # ÂâçÁ´ØÈùôÊÄÅÊñá‰ª∂
       location / {
           root /usr/share/nginx/html;
           try_files $uri $uri/ /index.html;
       }

       # API ‰ª£ÁêÜ
       location /api {
           proxy_pass http://backend;
           proxy_set_header Host $host;
           proxy_set_header X-Real-IP $remote_addr;
           proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
           proxy_set_header X-Forwarded-Proto $scheme;
       }

       # WebSocket ÊîØÊåÅ
       location /ws {
           proxy_pass http://backend;
           proxy_http_version 1.1;
           proxy_set_header Upgrade $http_upgrade;
           proxy_set_header Connection "upgrade";
       }
   }
   ```

3. **ÂàõÂª∫Áîü‰∫ßÁéØÂ¢É Docker Compose**
   ```yaml
   # docker-compose.prod.yml
   version: '3.8'

   services:
     postgres:
       image: postgres:15
       environment:
         POSTGRES_DB: stock_ai
         POSTGRES_USER: ${POSTGRES_USER}
         POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
       volumes:
         - postgres_data:/var/lib/postgresql/data
       restart: always

     redis:
       image: redis:7
       volumes:
         - redis_data:/data
       restart: always

     influxdb:
       image: influxdb:2.7
       environment:
         DOCKER_INFLUXDB_INIT_MODE: setup
         DOCKER_INFLUXDB_INIT_USERNAME: ${INFLUXDB_USER}
         DOCKER_INFLUXDB_INIT_PASSWORD: ${INFLUXDB_PASSWORD}
         DOCKER_INFLUXDB_INIT_ORG: stock-ai
         DOCKER_INFLUXDB_INIT_BUCKET: stock_data
         DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: ${INFLUXDB_TOKEN}
       volumes:
         - influxdb_data:/var/lib/influxdb2
       restart: always

     backend:
       build:
         context: ./backend
         dockerfile: Dockerfile.prod
       environment:
         - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/stock_ai
         - REDIS_HOST=redis
         - INFLUXDB_URL=http://influxdb:8086
       depends_on:
         - postgres
         - redis
         - influxdb
       restart: always

     celery_worker:
       build:
         context: ./backend
         dockerfile: Dockerfile.prod
       command: celery -A app.core.celery_app worker --loglevel=info
       environment:
         - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/stock_ai
         - REDIS_HOST=redis
       depends_on:
         - redis
         - postgres
       restart: always

     celery_beat:
       build:
         context: ./backend
         dockerfile: Dockerfile.prod
       command: celery -A app.core.celery_app beat --loglevel=info
       environment:
         - REDIS_HOST=redis
       depends_on:
         - redis
       restart: always

     frontend:
       build:
         context: ./frontend
         dockerfile: Dockerfile.prod
       ports:
         - "80:80"
       depends_on:
         - backend
       restart: always

   volumes:
     postgres_data:
     redis_data:
     influxdb_data:
   ```

4. **Êèê‰∫§‰ª£Á†Å**
   ```bash
   git add docker-compose.prod.yml backend/Dockerfile.prod frontend/Dockerfile.prod nginx/
   git commit -m "feat: add production Docker configuration"
   ```

---

### ‚¨ú Task 5: CI/CD ÊµÅÁ®ã
**Áä∂ÊÄÅ**: ÂæÖÂºÄÂßã
**Êñá‰ª∂**:
- ÂàõÂª∫: `.github/workflows/ci.yml`
- ÂàõÂª∫: `.github/workflows/deploy.yml`

**Ê≠•È™§**:

1. **ÂàõÂª∫ CI Â∑•‰ΩúÊµÅ**
   ```yaml
   # .github/workflows/ci.yml
   name: CI

   on:
     push:
       branches: [ main, develop ]
     pull_request:
       branches: [ main ]

   jobs:
     test-backend:
       runs-on: ubuntu-latest
       steps:
         - uses: actions/checkout@v3

         - name: Set up Python
           uses: actions/setup-python@v4
           with:
             python-version: '3.11'

         - name: Install dependencies
           run: |
             cd backend
             pip install -r requirements.txt

         - name: Run tests
           run: |
             cd backend
             pytest tests/ --cov=app --cov-report=xml

         - name: Upload coverage
           uses: codecov/codecov-action@v3
           with:
             file: ./backend/coverage.xml

     test-frontend:
       runs-on: ubuntu-latest
       steps:
         - uses: actions/checkout@v3

         - name: Set up Node.js
           uses: actions/setup-node@v3
           with:
             node-version: '18'

         - name: Install dependencies
           run: |
             cd frontend
             npm ci

         - name: Run tests
           run: |
             cd frontend
             npm test

         - name: Build
           run: |
             cd frontend
             npm run build

     lint:
       runs-on: ubuntu-latest
       steps:
         - uses: actions/checkout@v3

         - name: Lint backend
           run: |
             cd backend
             pip install ruff
             ruff check .

         - name: Lint frontend
           run: |
             cd frontend
             npm ci
             npm run lint
   ```

2. **ÂàõÂª∫ÈÉ®ÁΩ≤Â∑•‰ΩúÊµÅ**
   ```yaml
   # .github/workflows/deploy.yml
   name: Deploy

   on:
     push:
       branches: [ main ]

   jobs:
     deploy:
       runs-on: ubuntu-latest
       steps:
         - uses: actions/checkout@v3

         - name: Deploy to server
           uses: appleboy/ssh-action@master
           with:
             host: ${{ secrets.SERVER_HOST }}
             username: ${{ secrets.SERVER_USER }}
             key: ${{ secrets.SSH_PRIVATE_KEY }}
             script: |
               cd /opt/stock-ai
               git pull origin main
               docker-compose -f docker-compose.prod.yml down
               docker-compose -f docker-compose.prod.yml up -d --build
   ```

3. **Êèê‰∫§‰ª£Á†Å**
   ```bash
   git add .github/workflows/
   git commit -m "feat: add CI/CD workflows"
   ```

---

### ‚¨ú Task 6: ÈÉ®ÁΩ≤ÊñáÊ°£
**Áä∂ÊÄÅ**: ÂæÖÂºÄÂßã
**Êñá‰ª∂**:
- ÂàõÂª∫: `docs/DEPLOYMENT.md`

**Ê≠•È™§**:

1. **ÁºñÂÜôÈÉ®ÁΩ≤ÊñáÊ°£**
   ```markdown
   # ÈÉ®ÁΩ≤ÊåáÂçó

   ## ÁéØÂ¢ÉË¶ÅÊ±Ç

   - Docker 24+
   - Docker Compose 2.0+
   - 2 Ê†∏ CPU, 4GB RAMÔºàÊúÄ‰ΩéÔºâ
   - 50GB Á£ÅÁõòÁ©∫Èó¥

   ## ÈÉ®ÁΩ≤Ê≠•È™§

   ### 1. ÂÖãÈöÜ‰ª£Á†Å
   ```bash
   git clone https://github.com/your-org/stock-ai.git
   cd stock-ai
   ```

   ### 2. ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáè
   ```bash
   cp .env.example .env
   # ÁºñËæë .env Êñá‰ª∂ÔºåËÆæÁΩÆÊï∞ÊçÆÂ∫ìÂØÜÁ†Å„ÄÅAPI ÂØÜÈí•Á≠â
   ```

   ### 3. ÂêØÂä®ÊúçÂä°
   ```bash
   docker-compose -f docker-compose.prod.yml up -d
   ```

   ### 4. ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ì
   ```bash
   docker-compose exec backend alembic upgrade head
   ```

   ### 5. È™åËØÅÈÉ®ÁΩ≤
   ```bash
   curl http://localhost/api/v1/health
   ```

   ## ÁõëÊéß

   - Prometheus ÊåáÊ†á: http://localhost:8000/metrics
   - Celery ÁõëÊéß: http://localhost:5555

   ## Â§á‰ªΩ

   ### Êï∞ÊçÆÂ∫ìÂ§á‰ªΩ
   ```bash
   docker-compose exec postgres pg_dump -U stock_user stock_ai > backup.sql
   ```

   ### ÊÅ¢Â§ç
   ```bash
   docker-compose exec -T postgres psql -U stock_user stock_ai < backup.sql
   ```
   ```

2. **Êèê‰∫§‰ª£Á†Å**
   ```bash
   git add docs/DEPLOYMENT.md
   git commit -m "docs: add deployment guide"
   ```

---

## ÂÆåÊàêÊ†áÂáÜ

Phase 8 ÂÆåÊàêÂêéÔºåÁ≥ªÁªüÂ∫îÂÖ∑Â§á‰ª•‰∏ãËÉΩÂäõÔºö

### ÁõëÊéßÂÆåÊï¥ÊÄß
- ‚úÖ ÁªìÊûÑÂåñÊó•ÂøóÁ≥ªÁªü
- ‚úÖ Prometheus ÁõëÊéßÊåáÊ†á
- ‚úÖ ÂëäË≠¶Êú∫Âà∂
- ‚úÖ ÊÄßËÉΩÁõëÊéß

### ÈÉ®ÁΩ≤Â∞±Áª™
- ‚úÖ Áîü‰∫ßÁéØÂ¢É Docker ÈÖçÁΩÆ
- ‚úÖ Nginx ÂèçÂêë‰ª£ÁêÜ
- ‚úÖ CI/CD ÊµÅÁ®ã
- ‚úÖ ÈÉ®ÁΩ≤ÊñáÊ°£

### ËøêÁª¥ËÉΩÂäõ
- ‚úÖ Êó•ÂøóÊü•ËØ¢ÂíåÂàÜÊûê
- ‚úÖ ÊåáÊ†áÁõëÊéßÂíåÂëäË≠¶
- ‚úÖ Ëá™Âä®ÂåñÈÉ®ÁΩ≤
- ‚úÖ Â§á‰ªΩÂíåÊÅ¢Â§ç

---

## È°πÁõÆÂÆåÊàê

ÂÆåÊàê Phase 8 ÂêéÔºåÊï¥‰∏™È°πÁõÆÂºÄÂèëÂÆåÊàêÔºÅ

ÊâÄÊúâÊ†∏ÂøÉÂäüËÉΩÂ∑≤ÂÆûÁé∞Ôºö
- ‚úÖ ÈÄâËÇ°ÂºïÊìé
- ‚úÖ ‰∏™ËÇ°ÂàÜÊûê
- ‚úÖ AI Êô∫ËÉΩÂàÜÊûê
- ‚úÖ Êï∞ÊçÆÂêåÊ≠•
- ‚úÖ ÂâçÁ´ØÂ∫îÁî®
- ‚úÖ Áî®Êà∑ËÆ§ËØÅ
- ‚úÖ ÊÄßËÉΩ‰ºòÂåñ
- ‚úÖ ÁõëÊéßÈÉ®ÁΩ≤

È°πÁõÆÂèØ‰ª•ÊäïÂÖ•Áîü‰∫ß‰ΩøÁî®„ÄÇ
